{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Mise en Place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Géneration du Probléme-Jouet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_problem2d(Lambda):\n",
    "    A = np.random.rand(2,2)\n",
    "    Q,R = np.linalg.qr(A)\n",
    "    L = np.diag(Lambda)\n",
    "    P = Q.T@L@Q\n",
    "    # with P definite positive\n",
    "    q = np.random.rand(2)\n",
    "    r = np.random.rand(1) \n",
    "    return P,q,r\n",
    "\n",
    "def create_problem(m,n,scale,x0):\n",
    "    #centered a\n",
    "    A = scale*(np.random.rand(n,m)-.5)\n",
    "    b = x0@A + 5*scale*np.random.rand(m)\n",
    "    c = (np.random.rand(n)-.5)\n",
    "    return A,b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de Coût"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x):\n",
    "## f3\n",
    "    # diff = b - A.T @ x\n",
    "    \n",
    "    # if np.any(diff <= 0):\n",
    "    #     return np.inf\n",
    "    \n",
    "    # cout = c.T @ x - np.sum(np.log(diff))\n",
    "\n",
    "## f2\n",
    "    cout = np.exp(x[0] + 3*x[1] - 0.1) + np.exp(x[0] - 3*x[1] - 0.1) + np.exp(-x[0] - 0.1) \n",
    "\n",
    "## f1\n",
    "    # cout = 0.5 * (x.T @ P @ x) + q.T @ x + r\n",
    "    \n",
    "    return cout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x):\n",
    "## f3\n",
    "    # diff = b - A.T @ x\n",
    "    \n",
    "    # if np.any(diff <= 0):\n",
    "    #     return np.inf * np.ones_like(x)\n",
    "\n",
    "    # grad = c + A @ (1 / diff)\n",
    "\n",
    "\n",
    "## f2\n",
    "    grad = np.zeros(2)\n",
    "    grad[0] = np.exp(x[0] + 3*x[1] - 0.1) + np.exp(x[0] - 3*x[1] - 0.1) - np.exp(-x[0] - 0.1)\n",
    "    grad[1] = 3*np.exp(x[0] + 3*x[1] - 0.1) - 3*np.exp(x[0] - 3*x[1] - 0.1)\n",
    "\n",
    "## f1\n",
    "    # grad = P @ x + q\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessiene(x):\n",
    "## f3\n",
    "    # diff = b - A.T @ x\n",
    "    \n",
    "    # if np.any(diff <= 0):\n",
    "    #     return np.inf * np.eye(len(x))\n",
    "    \n",
    "    # hes = A @ np.diag(1 / (diff ** 2)) @ A.T\n",
    "\n",
    "## f2\n",
    "    hes = np.zeros((2,2))\n",
    "    hes[0,0] = np.exp(x[0] + 3 * x[1] - 0.1) + np.exp(x[0] - 3 * x[1] - 0.1) + np.exp(-x[0] - 0.1)\n",
    "    hes[1,1] = 9 * np.exp(x[0] + 3 * x[1] - 0.1) + 9 * np.exp(x[0] - 3 * x[1] - 0.1)\n",
    "    hes[1,0] = 3 * np.exp(x[0] + 3 * x[1] - 0.1) - 3 * np.exp(x[0] - 3 * x[1] - 0.1)\n",
    "    hes[0,1] = hes[1,0]\n",
    "\n",
    "## f1\n",
    "    # hes = P\n",
    "\n",
    "    return hes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backtracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backtracking(fonc, grad, d, x0):\n",
    "    sig = 1.0\n",
    "    c = 1e-4\n",
    "    rho = 0.5\n",
    "    \n",
    "    while fonc(x0 + sig*d) > fonc(x0) + c*sig*grad(x0).T @ d:\n",
    "        sig = rho*sig\n",
    "    \n",
    "    sigk = sig\n",
    "    \n",
    "    return sigk   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoldenSearch(Fonc, x0, direction):\n",
    "    a = 0\n",
    "    b = 2\n",
    "    Ep = 1e-12\n",
    "    phi = (1+np.sqrt(5))/2\n",
    "    iter_count = 0\n",
    "    max_iter = 100\n",
    "    \n",
    "    \n",
    "    # Lambda function to evaluate the cost function for a given step size sigma\n",
    "    f = lambda sigma: Fonc(x0 + sigma * direction)\n",
    "    \n",
    "    c = a + (b-a)/(phi+1)\n",
    "    d = b - (b-a)/(phi+1)\n",
    "       \n",
    "    while b - a > Ep and iter_count < max_iter:\n",
    "        if f(c) >= f(d):\n",
    "            a = c\n",
    "            c = d\n",
    "            d = b - (b-a)/(phi+1) \n",
    "        else: \n",
    "            b = d\n",
    "            d = c\n",
    "            c = a + (b-a)/(phi+1) \n",
    "        iter_count += 1  \n",
    "           \n",
    "    sig = (a + b) / 2       \n",
    "         \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode de Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newton(x0):\n",
    "\n",
    "    return np.linalg.inv(hessiene(x0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode de Descente de Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Descent(x0,cost,gradient,step, met):\n",
    "    n = np.linalg.norm(gradient(x0))\n",
    "    xlist = [x0]\n",
    "    flist = [cost(x0)]\n",
    "    nlist = [n]\n",
    "    k = 0\n",
    "    \n",
    "    while n >= 1e-6 and k < 100000:\n",
    "\n",
    "        grad = gradient(x0)\n",
    "        \n",
    "        if met == 'Pas Constant':\n",
    "            d = - step*grad\n",
    "        elif met == 'Pas Optimal':\n",
    "            sigma = GoldenSearch(cost, x0, -grad)\n",
    "            d = - sigma*grad\n",
    "        elif met == 'Backtrack':\n",
    "            sigma = Backtracking(cost, gradient, -grad, x0)\n",
    "            d = - sigma*grad\n",
    "        else:\n",
    "            hes = Newton(x0)\n",
    "            d = - hes@grad\n",
    "\n",
    "        x0 = x0 + d\n",
    "        k = k+1\n",
    "        n = np.linalg.norm(gradient(x0))\n",
    "        xlist.append(x0)\n",
    "        flist.append(cost(x0))\n",
    "        nlist.append(n)\n",
    "          \n",
    "    return xlist,flist,nlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation des Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions de differents Méthodes\n",
    "MET = np.array(['Newton', \n",
    "                'Backtrack', \n",
    "                'Pas Optimal',\n",
    "                'Pas Constant'\n",
    "                ])\n",
    "\n",
    "# Boucle pour chaque Méthode\n",
    "for met in MET:\n",
    "    print(met)\n",
    "    Lambda = np.array((1,2))\n",
    "    np.random.seed(seed=1)\n",
    "    P,q,r= quadratic_problem2d(Lambda)\n",
    "    #A,b,c = create_problem(500,2,2,x0) \n",
    "    # check solution existency \n",
    "    xstar = -np.dot(np.linalg.inv(P),q)\n",
    "    fmin  = cost(xstar)\n",
    "\n",
    "    #grid\n",
    "    ax= xstar[0]\n",
    "    bx = xstar[0]\n",
    "    ay= xstar[1]\n",
    "    by = xstar[1]\n",
    "\n",
    "    for test in range(2):\n",
    "        x0 = np.copy(xstar) + 2.*(np.random.rand(2)-.5)\n",
    "        #\n",
    "        start_time = time.time()\n",
    "        xlist,flist,nlist =  Gradient_Descent(x0,cost,gradient,1e-2, met)\n",
    "        RunTime = (time.time() - start_time) # Calcul du temps d'execution\n",
    "        print(\"Temps d'execution (secondes) for test n°\", test+1,  \": \", RunTime)\n",
    "        print(np.shape(nlist))\n",
    "        xlist = np.asarray(xlist)\n",
    "        #\n",
    "        fig = plt.figure(1) \n",
    "        plt.subplot(1,3,1)\n",
    "        plt.plot(xlist[:,0], xlist[:,1],'o-',label='points')\n",
    "    #    #\n",
    "        ax = np.min((xlist[:,0].min(),ax))-.1\n",
    "        bx = np.max((xlist[:,0].max(),bx))+.1\n",
    "        ay = np.min((xlist[:,1].min(),ay))-.1\n",
    "        by = np.max((xlist[:,1].max(),by))+.1\n",
    "        \n",
    "        print(Lambda)\n",
    "        print(flist)\n",
    "        print(fmin)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.semilogy(range(len(flist)),flist-fmin+1e-16)\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel(r'$f(x^k)$')\n",
    "        \n",
    "        plt.subplot(1,3,3)\n",
    "        plt.semilogy(nlist,':')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel(r'$f(x^k)$')\n",
    "        \n",
    "    #    \n",
    "    xgrid = np.arange(ax,bx,(bx-ax)/50)\n",
    "    ygrid = np.arange(ay,by,(by-ay)/50)\n",
    "    X, Y = np.meshgrid(xgrid, ygrid)\n",
    "    Z = np.zeros(X.shape)\n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            Z[i,j] = cost(np.array([X[i,j],Y[i,j]]))\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.contour(X, Y, Z,21)\n",
    "\n",
    "    plt.plot(xstar[0], xstar[1],'*',label='points')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
